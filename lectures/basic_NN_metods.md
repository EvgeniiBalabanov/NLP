# Basic neural network methods for working with texts

1. **Общий алгоритм работы**
    - Подготовка текста
    > разбить на элементы, построить словарь, перенумеровать
    - Получение базовых векторных представлений
    > Embeddings(вложения) слов и символов, построение матрицы текста
    - Преобразование матрицы текста
    > Учёт локального контекста
    - Агрегация
    > Весь текст сжимается в вектор
    - Решение конкретной задачи 
---
2. Матрица совместной встречаемости
    - Она является квардратная, n x n
    - А так же симетричная

    ||мама|любить|молоток|забивать
    ---|---|---|---|---
    мама|-|10^5|10^2|10
    любить|10^5|-|10|10^3
    молоток|10^2|10|-|10^6
    забивать|10|10^3|10^6|-

    Общий алгоритм:
    - Построить матрицу совметной встречаемости размера

            X_(|Vocabulary| x |Vocabulary|)
    - Сгладить веса в матрице 
        * Cчётчики могут отличаться на несколько порядков
        * В качестве функции сглаживания, может использоваться логарифмирование:
            >log(1+x_ij)
        * Частотные слова вносят больший вклад
        * Точечная взаимная информация PPMI
    - Построить аппроксимацию этой матрицы
            
            X = W_(|Vocabulary| x EmbSize) * D_(EmbSize x |Vocabulary|)